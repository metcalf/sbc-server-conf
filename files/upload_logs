#!/bin/bash
set -euxo pipefail

if [ "$(id -u)" -eq 0 ]; then
    chgrp --recursive loguploader /var/log/remote
    exec sudo -H -u loguploader $0
fi

trap '~/heartbeat /$?' EXIT

# Change to the home directory so `find` doesn't error trying to change back to /root
cd ~

# Iterate over log files that are ready to upload and upload them
# We don't use `aws s3 sync` to avoid needing the GetObject permission
/usr/bin/find /var/log/remote -name '*.log-*.gz' -print0 | while IFS= read -r -d "" logpath; do
    logname="$(basename $logpath)"
    s3path="s3://itsshedtime-logs/home-sbc-server/$logname"

    if aws s3 ls "$s3path"
    then
        echo "$logname is already on S3"
    else
        aws s3 cp "$logpath" "$s3path"
        echo "uploaded $logname"
    fi
done
