#!/bin/bash
set -euxo pipefail

if [ "$(id -u)" -eq 0 ]; then
    chgrp --recursive loguploader /var/log/remote
    exec sudo -H -u loguploader $0
fi

trap 'exit_code=$?; ~/heartbeat /$exit_code; exit $exit_code' EXIT

# Change to the home directory so `find` doesn't error trying to change back to /root
cd ~

# Iterate over log files that are ready to upload and upload them
# We don't use `aws s3 sync` to avoid needing the GetObject permission
/usr/bin/find /var/log/remote -name '*.log-*.gz' -print0 | while IFS= read -r -d "" logpath; do
    logname="$(basename "$logpath")"

    # Extract the date part using the known position from the end
    # Removes .gz extension, then takes the last 8 characters
    date_str=${logname%.gz}
    date_str=${date_str: -8}

    # Extract year and month
    year=${date_str:0:4}
    month=${date_str:4:2}

    s3path="s3://itsshedtime-logs/home-sbc-server/$year/$month/$logname"

    if aws s3 ls "$s3path"
    then
        echo "$logname is already on S3"
    else
        aws s3 cp "$logpath" "$s3path"
        echo "uploaded $logname"
    fi
done
